:data-uri:
:toc2:
:linkattrs:
:tower_username: babylon
:tower_password: changeme
:organization_name: rhte
:project_name: babylon
:inventory_name: empty-inventory
:credential_name: babylon-tower-credential


[numbered]

== Ansible Tower Infrastructure as Code With Project Babylon Hands-On Lab

In this lab you will use Ansible Tower Cluster, Workflows, and Isolated Nodes to allow Ansible based `configs` to be simply deployed at scale globally and provide rapid and robust complex deployments.

Combined with Infrastructure as Code Best Practices this creates a uniquely flexible deployment platform
allowing multiple disparate teams to deploy and teardown highly customized Cloud Environments
simply, flexibly, rapidly, and repeatably.

.Goal

* Use distributed multi region Ansible Tower Cluster.
* Use Ansible `tower-cli` and modules
* Use Ansible Tower API requests
* Ansible Tower Isolated Node group for scaling Ansible tower cluster globally
* Infrastructure as Code Best Practices

:numbered:

== Review Provisioned Environment Hosts

=== In this section, you will explore the lab environment.

The lab consists of `four` internal servers behind a bastion host or jumpbox. A Ansible tower server, tower1, is exposed for HTTP and HTTPS traffic.

At this point, Ansible Tower is deployed on the servers.

* Bastion server: bastion.${GUID}.internal, bastion.${GUID}.example.opentlc.com

* Ansible Tower nodes: tower1.${GUID}.example.opentlc.com, tower1.${GUID}.internal

* External primary PostgreSQL database: support1.${GUID}.example.opentlc.com, support1.${GUID}.internal

* Ansible Tower Isolated Nodes: worker{1}.{emea,na}.example.opentlc.com, worker{1}.{emea,na}.internal

You will work and run your playbooks directly from bastion.

===  Get familiar with prompt name convention

Certain tasks below must be executed on the bastion host. Code and command examples indicate which host to use in the prompt:

* On your laptop: `[laptop ]$`

* On the bastion host: `[lab-user@bastion ~]$  sudo -i`

* On the bastion host: `[root@bastion ~]#`

== Explore and Verify Environment

In this section, you verify that the infrastructure
and hosts needed for Ansible Tower exist and that you can access them.
Instances are created for you, and you can verify that they are accessible from
the `bastion` host.

=== Connect to Environment

* From your laptop, connect to `bastion` host.
** Username: `lab-user`
** Password: `r3dh4t1!`

+
[source,test]
----
[laptop]$ export GUID=<from GUIDgrabber>
[laptop]$ ssh lab-user@bastion.$GUID.sandbox.example.opentlc.com
[lab-user@bastion 0 ~]$
----

* As root, use the `ansible all --list-hosts` command to list the available hosts:

+
[source,text]
----
[lab-user@bastion 0 ~]$ sudo -i
[root@bastion 0 ~]# ansible all --list-hosts
----

* Sample Output

+
[source,text]
----
support1.5af3.internal
worker1.na.5af3.internal
tower2.5af3.internal
tower1.5af3.internal
worker1.emea.5af3.internal
----

* Using the Fully Qualified Domain name of the bastion host you used in your ssh command do the following:
Grab info from bastion public name to setup lab-user variable GUID & BUID .

+
[source,text]
----
[root@bastion 0 ~]# export GUID=$(echo bastion.484c.sandbox712.opentlc.com | awk -F "." '{print $2}')
[root@bastion 0 ~]# export BUID=$(echo bastion.484c.sandbox712.opentlc.com | awk -F "." '{print $3}')
----

NOTE: Make sure you use *your* FQDN from your ssh command and not that in the lab example!

* Sample Output
+
[source,text]
----
[root@bastion 0 ~]# echo $GUID
5af3
[root@bastion 0 ~]# echo $BUID
sandbox586
----



* Use the Ansible `ping` command to verify that all of your hosts are running:
+
[source,sh]
----
[root@bastion 0 ~]# ansible all -m ping
----

* Sample Output
+
[source,text]
----
worker1.emea.GUID.internal | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python"
    },
    "changed": false,
    "ping": "pong"
}
worker1.na.GUID.internal | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python"
    },
    "changed": false,
    "ping": "pong"
}
tower1.GUID.internal | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python"
    },
    "changed": false,
    "ping": "pong"
}
tower2.GUID.internal | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python"
    },
    "changed": false,
    "ping": "pong"
}
support1.GUID.internal | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python"
    },
    "changed": false,
    "ping": "pong"
}
----

== Explore Ansible Tower UI

=== Use the public IP address or DNS name to access the `tower1` Ansible node.

In this section, you login to tower to explore.

* Open your browser to access following tower url `https://tower1.${GUID}.${BUID}.opentlc.com/`.
** Replace `GUID` and `BUID` value from the previous steps before copying the url.

* When you receive a warning from your browser that the Ansible Tower serverâ€™s
security certificate is not secure, add and confirm the security exception for the
self-signed certificate.

* Log in to the Ansible Tower web interface as the administrator using the `{tower_username}`
account and `{tower_password}` as the password.


=== List Clustered Nodes
In this section, you list all of the nodes in the Ansible Tower cluster.


* Click on `Instance Groups` option under `ADMINISTRATION` section from menu. (Lower left hand of the screen)

* Click on `tower` from instance groups.

* Click on `INSTANCES` from tower groups dialog to list nodes.

* Similarly explore rest left instance groups to list nodes.


=== View Users
In this section, you list users on the Ansible Tower.

* Click on `Users` option under `ACCESS` section from menu.

* Click on `{tower_username}` for details.


=== List/sync Projects

In this section, you list projects on Ansible Tower and sync project called `{project_name}`.

* Click on `Projects` option under `RESOURCES` section from menu.

* Click on project `{project_name}` to view details.

* To sync project go back to project page and click on sync icon respective to project `{project_name}` to get latest SCM version.


=== List Inventories

In this section, you list all inventories on Ansible Tower.

* Click on `Inventories` option under `RESOURCES` section from menu to list all inventories.


=== List Credentials

In this section, you list credentials.

* Click on `Credentials` under `RESOURCES` from the menu.

* Click `{credential_name}` for details



== Explore Ansible tower cli

=== View tower-cli config

* In this section, you view tower-cli command configuration settings.
As the root user on the bastion execute the following:
+
[source,sh]
----
[root@bastion 1 ~]# tower-cli config
----

* Sample Output
+
[source,text]
----
# User options (set with `tower-cli config`; stored in ~/.tower_cli.cfg).
host: tower1.$GUID.internal
username: admin
password: changeme
verify_ssl: False

# Defaults.
use_token: False
verbose: False
certificate:
format: human
color: True
insecure: False
description_on: False
oauth_token:
----


=== List Resources
In this section, you list various Ansible Tower resource.

* List Users
+
[source,sh]
----
[root@bastion 1 ~]# tower-cli user list
----

* Sample Output
+
[source,text]
----
== ============== ==================== ========== ========= ============ =================
id    username           email         first_name last_name is_superuser is_system_auditor
== ============== ==================== ========== ========= ============ =================
 1 admin          admin@example.com                                 true             false
 3 babylon-viewer babylon1@example.com Babylon    Viewer           false             false
 2 babylon        babylon@example.com  Baby       Lon               true             false
== ============== ==================== ========== ========= ============ =================
----

* List Job Templates
+
[source,sh]
----
[root@bastion 1 ~]# tower-cli job_template list
----

* Sample Output
+
[source,text]
----
== ============== ========= ======= ==============
id      name      inventory project    playbook
== ============== ========= ======= ==============
 7 job-runner         4       6 job-runner.yml
== ============== ========= ======= ==============
----

== Explore Ansible Tower REST API with Curl

=== List of HA nodes
* In this section, you list HA nodes and Instance groups.

+
[source,sh]
----
[root@bastion 0 ~]# curl -sk -u babylon:changeme https://tower1.$GUID.$BUID.opentlc.com/api/v2/ping/ | jq
----

* Sample Output
+
[source,text]
----
{
  "ha": true,
  "version": "3.5.0",
  "active_node": "tower1.$GUID.internal",
  "install_uuid": "67ce062e-11de-486d-a3ee-886b5f4982ce",
  "instances": [
    {
      "node": "worker1.emea.$GUID.internal",
      "uuid": "ebb427c8-fdf6-4d0d-b866-cade01e49f60",
      "heartbeat": "2019-09-02T08:52:48.624811Z",
      "capacity": 17,
      "version": "ansible-runner-1.3.4"
    },
    {
      "node": "worker1.na.$GUID.internal",
      "uuid": "ebb427c8-fdf6-4d0d-b866-cade01e49f60",
      "heartbeat": "2019-09-02T08:52:48.630539Z",
      "capacity": 17,
      "version": "ansible-runner-1.3.4"
    },
    {
      "node": "tower1.$GUID.internal",
      "uuid": "ebb427c8-fdf6-4d0d-b866-cade01e49f60",
      "heartbeat": "2019-09-02T08:56:19.758708Z",
      "capacity": 17,
      "version": "3.5.0"
    },
    {
      "node": "tower2.$GUID.internal",
      "uuid": "86060ac6-a74d-4e75-9e68-6e983e36b429",
      "heartbeat": "2019-09-02T08:57:12.198288Z",
      "capacity": 17,
      "version": "3.5.0"
    }
  ],
  "instance_groups": [
    {
      "name": "tower",
      "capacity": 34,
      "instances": [
        "tower1.$GUID.internal",
        "tower2.$GUID.internal"
      ]
    },
    {
      "name": "na",
      "capacity": 17,
      "instances": [
        "worker1.na.$GUID.internal"
      ]
    },
    {
      "name": "emea",
      "capacity": 17,
      "instances": [
        "worker1.emea.$GUID.internal"
      ]
    }
  ]
}

----

=== List Users
* In this section, you list users.


+
[source,sh]
----
[root@bastion 0 ~]# curl -sk -u babylon:changeme https://tower1.$GUID.$BUID.opentlc.com/api/v2/users/ | jq
----

* Sample Output
+
[source,text]
----
{
  "count": 3,
  "next": null,
  "previous": null,
  "results": [
    {
      "id": 1,
      "type": "user",
      "url": "/api/v2/users/1/",
      "related": {
        "teams": "/api/v2/users/1/teams/",
        "organizations": "/api/v2/users/1/organizations/",
        "admin_of_organizations": "/api/v2/users/1/admin_of_organizations/",
        "projects": "/api/v2/users/1/projects/",
        "credentials": "/api/v2/users/1/credentials/",
        "roles": "/api/v2/users/1/roles/",
        "activity_stream": "/api/v2/users/1/activity_stream/",
        "access_list": "/api/v2/users/1/access_list/",
        "tokens": "/api/v2/users/1/tokens/",
        "authorized_tokens": "/api/v2/users/1/authorized_tokens/",
        "personal_tokens": "/api/v2/users/1/personal_tokens/"
      },
      "summary_fields": {
        "user_capabilities": {
          "edit": true,
          "delete": true
        }
      },
      "created": "2019-09-02T02:11:52.556992Z",
      "username": "admin",
      "first_name": "",
      "last_name": "",
      "email": "admin@example.com",
      "is_superuser": true,
      "is_system_auditor": false,
      "ldap_dn": "",
      "last_login": null,
      "external_account": null,
      "auth": []
    },
    {
      "id": 3,
      "type": "user",
      "url": "/api/v2/users/3/",
      "related": {
        "teams": "/api/v2/users/3/teams/",
        "organizations": "/api/v2/users/3/organizations/",
        "admin_of_organizations": "/api/v2/users/3/admin_of_organizations/",
        "projects": "/api/v2/users/3/projects/",
        "credentials": "/api/v2/users/3/credentials/",
        "roles": "/api/v2/users/3/roles/",
        "activity_stream": "/api/v2/users/3/activity_stream/",
        "access_list": "/api/v2/users/3/access_list/",
        "tokens": "/api/v2/users/3/tokens/",
        "authorized_tokens": "/api/v2/users/3/authorized_tokens/",
        "personal_tokens": "/api/v2/users/3/personal_tokens/"
      },
      "summary_fields": {
        "user_capabilities": {
          "edit": true,
          "delete": true
        }
      },
      "created": "2019-09-02T02:17:31.684646Z",
      "username": "babylon-viewer",
      "first_name": "Babylon",
      "last_name": "Viewer",
      "email": "babylon1@example.com",
      "is_superuser": false,
      "is_system_auditor": false,
      "ldap_dn": "",
      "last_login": null,
      "external_account": null,
      "auth": []
    },
    {
      "id": 2,
      "type": "user",
      "url": "/api/v2/users/2/",
      "related": {
        "teams": "/api/v2/users/2/teams/",
        "organizations": "/api/v2/users/2/organizations/",
        "admin_of_organizations": "/api/v2/users/2/admin_of_organizations/",
        "projects": "/api/v2/users/2/projects/",
        "credentials": "/api/v2/users/2/credentials/",
        "roles": "/api/v2/users/2/roles/",
        "activity_stream": "/api/v2/users/2/activity_stream/",
        "access_list": "/api/v2/users/2/access_list/",
        "tokens": "/api/v2/users/2/tokens/",
        "authorized_tokens": "/api/v2/users/2/authorized_tokens/",
        "personal_tokens": "/api/v2/users/2/personal_tokens/"
      },
      "summary_fields": {
        "user_capabilities": {
          "edit": true,
          "delete": false
        }
      },
      "created": "2019-09-02T02:17:26.939682Z",
      "username": "babylon",
      "first_name": "Baby",
      "last_name": "Lon",
      "email": "babylon@example.com",
      "is_superuser": true,
      "is_system_auditor": false,
      "ldap_dn": "",
      "last_login": "2019-09-02T07:49:52.138941Z",
      "external_account": null,
      "auth": []
    }
  ]
}
----


== Setup AWS SSH key
=== Install `boto3` and `awscli`.
In this section, you will create a key pair to log in to your instances. You will make `agnosticd` the deployer
  tool using `rhte-miniV` variable repo for configuring instances.

* Install boto3 and awscli using pip.

** All of the AWS modules require recent versions of boto.
** awscli is cli tool to manage AWS api.

+
[source,sh]
----
# pip install boto3 awscli -U
----

** For more information refer to link:https://docs.ansible.com/ansible/latest/scenario_guides/guide_aws.html[guide_aws] .

=== Create AWS Keypair

* Run playbook to create AWS ssh keypair in `eu-central-1` and `ap-southeast-2` regions.

** For more information refer to link:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html[ec2-key-pair] .

+
[source,sh]
----
[root@bastion 0 ~]# cat << EOF > gpte_aws_keypair.yaml
- hosts: localhost
  gather_facts: false
  tasks:
  - name: Generate ssh keypair
    openssh_keypair:
      path: /root/.ssh/gpte
      size: 2048
      type: rsa
      force: yes
  - name: create key pair using key_material obtained using 'file' lookup plugin
    ec2_key:
      name: gpte
      key_material: "{{ lookup('file', '/root/.ssh/gpte.pub') }}"
      region: "{{ item }}"
      force: yes
    loop:
      - eu-central-1
      - ap-southeast-2
  - name: List Key pairs
    shell: aws ec2 describe-key-pairs --key-name gpte --region "{{ item }}"
    loop:
      - eu-central-1
      - ap-southeast-2
    register: keypair
  - name: Keypair List
    debug: var=keypair

- hosts: tower:isolated_group_emea:isolated_group_na
  gather_facts: false
  tasks:
  - name: Copy ssh private key
    copy:
      src: /root/.ssh/gpte
      dest: /var/lib/awx/.ssh/gpte.pem
      mode: 0400
      owner: awx
      group: awx

EOF
----

* Run the playbook

+
[source,sh]
----
[root@bastion 130 ~]# ansible-playbook gpte_aws_keypair.yaml
----

* Sample, truncated, Output

+
[source,sh]
----
"item": "eu-central-1",
                 "stdout_lines": [
                    "{",
                    "    \"KeyPairs\": [",
                    "        {",
                    "            \"KeyName\": \"gpte\", ",
                    "            \"KeyFingerprint\": \"b7:57:f0:66:53:12:71:ca:96:e4:f8:fb:be:f2:78:99\"",
                    "        }",
                    "    ]",
                    "}"


"item": "ap-southeast-2",
                stdout_lines": [
                    "{",
                    "    \"KeyPairs\": [",
                    "        {",
                    "            \"KeyName\": \"gpte\", ",
                    "            \"KeyFingerprint\": \"b7:57:f0:66:53:12:71:ca:96:e4:f8:fb:be:f2:78:99\"",
                    "        }",
                    "    ]",
                    "}"
----



== Setup Infrastructure as Code "Variable" Repo rhte-miniV

In this section, we will setup `rhte-miniV` repo used by `agnosticd` deployer.



The repo contains the variables needed for catalog items needed to be deployed.

* Goals
** Separate code and data
** Deploy the same way we develop, using a yaml file.
** Easy on-boarding of lab and demo creators
** Contain catalog information (description, â€¦â€‹)
** Automatically generate catalog
** Lab creators and Administrator will easily contribute to agnosticV
** Easy maintenance (YAML files in a git repo)
** Introduce good processes for contributing
*** Pull request
*** Peer review
** Automatic tests
** Syntax checking, Linters
** Enforce policies (ex: "catalog item must have a description")
** Detect non-documented variables
** Risk mitigation: no more manual operations on the deployment servers

=== AgnosticD Explained

* Ansible Agnostic Deployer, AKA AgnosticD, is a fully automated 2 Phase deployer for building and deploying everything from basic infrastructure to fully configured running application environments running on either public Cloud Providers or OpenShift clusters.


* For more information click link:https://github.com/redhat-cop/agnosticd/blob/development/README.adoc[agnosticD]

=== Clone `rhte-miniV` repo.

* Run `git` command to clone repo.

+
[source,sh]
----
[root@bastion 0 ~]# git clone https://github.com/redhat-gpte-devopsautomation/rhte-miniV.git
----

* Sample Output

+
[source,sh]
----
Cloning into 'rhte-miniV'
remote: Enumerating objects: 36, done.
remote: Counting objects: 100% (36/36), done.
remote: Compressing objects: 100% (30/30), done.
remote: Total 36 (delta 10), reused 32 (delta 6), pack-reused 0
Unpacking objects: 100% (36/36), done.
----

* Click the url link:https://github.com/redhat-gpte-devopsautomation/rhte-miniV.git[rhte-miniV] and review readme.adoc to understand the structure of rhte-miniV.

[NOTE]
`rhte-miniV` is sub set of agnosticV we are using in Production 
Babylon project. `rhte-miniV` was just created for this event.

== Setup values of variables in rhte-miniV repo for gpte organization.

=== Setup `/root/rhte-miniV/gpte/common.yaml`

* You will populate `/root/rhte-miniV/gpte/common.yaml` the values of the variables which are common to `gpte` organization.

** List Hosted zones for zone id
** HostedZoneId: The ID of the private hosted zone that you want to associate an Amazon VPC with.

** For more information: link:https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zones-private.html[hosted-zones] and link:https://docs.aws.amazon.com/cli/latest/reference/route53/list-hosted-zones.html[list-hosted-zones]


*** Create a playbook

+
[source,sh]
----
# cat << EOF > route53-zone-list.yaml
- name: List route53 hosted zones
  hosts: localhost
  gather_facts: false

  tasks:
    - name: List all hosted zones
      route53_facts:
        query: hosted_zone
      register: r_hosted_zones

    - name: List Route53 zones
      debug:
        msg:
         - " {{ item[0].Id }}"
         - " {{ item[0].Name }}"
      loop:
        - "{{ r_hosted_zones.HostedZones }}"
EOF
----

*** Run the playbook

+
[source,sh]
----
[root@bastion 0 ~]# ansible-playbook route53-zone-list.yaml
----

*** Sample Output

+
[source,sh]
----
"item[0].Id": "/hostedzone/Z12ZVUBFMBSIOW"
"item[0].Name": "sandbox422.opentlc.com."
----

[NOTE]
We will be using HostzoneID and Domain name from Route53(AWS DNS service) to create A records for our AWS instances.

** Retrieve your sandbox's AWS credential from `/root/.aws/credentials`.

+
[source,sh]
----
[root@bastion 0 ~]# cd rhte-miniV/
[root@bastion 0 ~/rhte-miniV/gpte master â­‘|âœ”]# cat /root/.aws/credentials
----

* Sample Output

+
[source,sh]
----
[default]
aws_access_key_id = Aklsjflksajflksj21312jsdfjK
aws_secret_access_key = Jsdfsdfds231fn
----

** Populate `/root/rhte-miniV/gpte/common.yaml` with the information which you rertieved from previous steps.

** For example

+
[source,sh]
----
[root@bastion 0 ~/rhte-miniV/gpte master â­‘|âœš1]# vi /root/rhte-miniV/gpte/common.yaml

HostedZoneId:                 Z12ZVUBFMBSIOW             ### Value of "item[0].Id" after /hostedzone/
subdomain_base_suffix:        .sandbox422.opentlc.com    ### value of "item[0].Name" prefixed with `.`
key_name:                     gpte                       ### Keyname you have created eralier
aws_access_key_id:            Akj234ssfs342jkfjdsK       ### Retreive from /root/.aws/credentials
aws_secret_access_key:        Jsfjksdhf2423423521fsffkjsdhfkjhsdjfhn
----

[NOTE]
Please do not forget to prefix subdomain_base_suffix value with the `.` e.g. `subdomain_base_suffix:        .sandbox422.opentlc.com`

[TIP]
In `vi` editor you can switch to the Insert mode from the command mode by pressing 'i' on the keyboard.  To save the changes you have made you need to press the `Esc key` and then `:x` to write and quit.



== Deploy OCP Client VM config

In this section you will populate `/root/rhte-miniV/gpte/OCP_CLIENTVM/{common,dev}.yaml` for `OCP_CLIENTVM` config which deploys the OpenShift Client VM used in many OpenShift courses and workshops.

=== Setup `OCP_CLIENTVM` config related variables.

Each `config` has it's own `common.yaml` file where _common_ varaibles are set which can be over-ridden by *stage* var files such as `dev.yaml` or `prod.yaml` at deployment time.

* Here you you will populate `/root/rhte-miniV/gpte/OCP_CLIENTVM/common.yaml` the values of the variables which are common for `OCP_CLIENTVM` config.
** Set the value of `action` to `deploy` for deploying a new config if necessary at the bottom of the file.

** For example

+
[source,sh]
----
[root@bastion 0 ~]# vi /root/rhte-miniV/gpte/OCP_CLIENTVM/common.yaml
tower:
    organization:             gpte
    run_group:                na  # e.g.region hint, stage hint (dev|prod)
    action:                   deploy
----

* You will populate `/root/rhte-miniV/gpte/OCP_CLIENTVM/dev.yaml` the values of the variables which are meant for `dev` catalog item `OCP_CLIENTVM` config.

** Set the values of the following:
+
|=====
|own_repo_path | http://d3s3zqyaz8cp2d.cloudfront.net/repos/ocp/3.9.51
|aws_region | ap-southeast-2
|guid | rhte1
|=====

** For example:

+
[source,sh]
----
[root@bastion 0 ~]# vi /root/rhte-miniV/gpte/OCP_CLIENTVM/dev.yaml
own_repo_path:                http://d3s3zqyaz8cp2d.cloudfront.net/repos/ocp/3.9.51
cloud_provider:               ec2
aws_region:                   ap-southeast-2
guid:                         rhte1
----

TIP: Take care not to delete other variables in the file

=== Merge variable files

** Merging strategy
** If a variable is defined in several YAML files, the definition in the last file has precedence over the others.
** When a variable is present in more than one file:
** if itâ€™s a string, last definition will override previous
** if itâ€™s a list, last definition will override previous
** if itâ€™s a dictionary , all dictionaries will be merged

[TIP]
Click the url https://github.com/redhat-gpte-devopsautomation/rhte-miniV.git to read Merging strategy.



=== Install `yq` for `./babylon-merge.sh` script.

** Script `./babylon-merge.sh` uses `yq` command line processor. Create the playbook to install `yq`.

+
[source,sh]
----
[root@bastion 0 ~]# cat << EOF > install_yq.yaml
- hosts: localhost
  gather_facts: false
  become: yes
  tasks:
  - name: Install Yq
    get_url:
      url: https://github.com/mikefarah/yq/releases/download/2.4.0/yq_linux_amd64
      dest: /usr/bin/yq
      mode: 0755
      owner: root
      group: root
EOF
----

** Run the playbook

+
[source,sh]
----
[root@bastion 0 ~]# ansible-playbook /root/install_yq.yaml
----

** Test `yq` is installed.
+

[source,sh]
----
[root@bastion 0 ~]# yq --version
----

** Sample Output
+

[source,sh]
----
yq version 2.4.0
----

=== Merge variable files.

** Run the script `./babylon-merge.sh` to

+
[source,sh]
----
[root@bastion 0 ~]# cd /root/rhte-miniV/
[root@bastion 0 ~/rhte-miniV master â­‘|âœš4â€¦2]# ./babylon-merge.sh gpte OCP_CLIENTVM dev > /root/ocp-clientvm.yml
[root@bastion 0 ~/rhte-miniV master â­‘|âœš4â€¦2]# cd
----

[TIP]
If you make any change in the *.yaml files then you need to run `./babylon-merge.sh` script again to generate merged var file.

** Explore the file `ocp-clientvm.yml`.

+
[source,sh]
----
[root@bastion 0 ~]# less /root/ocp-clientvm.yml
----

** Sample Output

+
[source,yaml]
----
---
job_vars:
  job_vars:
  __meta__:
    callback:
      token: ""
      url: ""
    catalog:
      description: Install OCP Client VM
      namespace: openshift
      parameters:
      - description: OCP version
        name: osrelease
        value: 3.9.51
      tags:
      - babylon
      - ocp
    deployer:
      entry_point: ansible/main.yml
      scm_ref: ocp4-client-vm-0.1
      scm_tag_prefix: ocp4-client-vm
      scm_type: git
      scm_url: https://github.com/redhat-cop/agnosticd.git
      type: agnosticd
    tower:
      action: deploy
      organization: gpte
      run_group: na
  HostedZoneId: Z7BVC200TM0XQ
  agnosticv_meta:
    agnosticd_git_repo: https://github.com/redhat-cop/agnosticd
          user: student_name
  aws_access_key_id: AajhdjaadasdasdasdashdjhasjdhaksK
  aws_secret_access_key: Ja;dlkas;lkda;lskd;laskdlakssdhn
  clientvm_instance_type: t2.medium
  cloud_provider: ec2
  cloudformation_retries: 0
  email: babylon@example.com
  env_type: ocp-clientvm
  guid: rhte1
----

=== Deploy `OCP_CLIENTVM`

* Run `tower-cli` to deploy env

[source,sh]
----
[root@bastion 0 ~]# tower-cli job launch --job-template=job-runner -e @/root/ocp-clientvm.yml
----

[TIP]
-vv for verbose output and --monitor allows you to monitor the job logs and look for errors. It is a good way to troubleshoot the issues.

* Sample output
+
[source,sh]
----
Resource changed.
== ============ =========================== ======= =======
id job_template           created           status  elapsed
== ============ =========================== ======= =======
13            7 2019-09-18T01:37:27.037967Z pending 0.0
== ============ =========================== ======= =======
----


* Explore Ansible Tower Web-UI `https://tower1.${GUID}.${BUID}.opentlc.com/`

** Login with user: `babylon` and password: `r3dh4t1!`.

** Click on Projects from left side pane. You will able to see a new project with the name <type>-<scm_ref> for example `agnosticd-ocp4-client-vm-0.1`.

** Click on Templates a new job template is created for deployment with the name <action>-<type>-<scm_ref> for example `deploy-agnosticd-ocp4-client-vm-0.1`.

** CLick on Jobs and you will see two active jobs `job-runner` and `deploy-agnosticd-ocp4-client-vm-0.1`.

** Click on `deploy-agnosticd-ocp4-client-vm-0.1` job to view the deployment logs.

TIP: You can retrieve the status of your job with `tower-cli job status <JOB-ID>` Once complete you can retrieve the logs with `tower-cli job stdout <JOB-ID>`

=== Anarchy Operator

** In the lab we are doing manual steps which `Anarchy` will be doing for us

*** `babylon-merge.sh`: Merging variable yaml files.
*** `tower-cli`: To deploy config.

** Merging variables will be handled by Anarchy operator on OCP cluster.
** For more information click link:https://github.com/redhat-gpte-devopsautomation/anarchy-operator[anarchy]
** Anarchy operator will be merging all the yaml files in AgnosticV in the Babylon Project which will be passed as extra variables to `tower-cli` command.
** Anarchy Operator after merging the vars will be deploying the config using Red Hat Ansible Tower for example: OCP CLient VM or Ansible Tower.


== Ansible Tower config

=== Setup `ANSIBLE_TOWER` config related variables.

In this section you will populate `/root/rhte-miniV/gpte/ANSIBLE_TOWER/{common,dev}.yaml` for `ANSIBLE_TOWER` config.

* You will populate `/root/rhte-miniV/gpte/ANSIBLE_TOWER/common.yaml` the values of the variables which are common for `ANSIBLE_TOWER` config.
** Set the value of `action` to `deploy` for deploying new config.

** For example:

+
[source,sh]
----
[root@bastion 0 ~]# vi /root/rhte-miniV/gpte/ANSIBLE_TOWER/common.yaml
tower:
    organization:             gpte
    run_group:                emea  # e.g.region hint, stage hint (dev|prod)
    action:                   deploy
----

* You will populate `/root/rhte-miniV/gpte/ANSIBLE_TOWER/dev.yaml` the values of the variables which are meant for `dev` catalog item `ANSIBLE_TOWER` config.

** Set the values of the following:
+
|=====
|own_repo_path | http://d3s3zqyaz8cp2d.cloudfront.net/repos/tower
|aws_region | eu-central-1
|guid | rhte1
|=====

** For example:
+

[source,sh]
----
[root@bastion 0 ~]# vi /root/rhte-miniV/gpte/ANSIBLE_TOWER/dev.yaml
own_repo_path:                http://d3s3zqyaz8cp2d.cloudfront.net/repos/tower
cloud_provider:               ec2
aws_region:                   eu-central-1
guid:                         rhte2
----

=== Merge the variables

** Run the script `./babylon-merge.sh` to merge variables

+
[source,sh]
----
[root@bastion 0 ~]# cd /root/rhte-miniV/
[root@bastion 0 ~/rhte-miniV master â­‘|âœš4â€¦2]# ./babylon-merge.sh gpte ANSIBLE_TOWER dev > /root/ansible-tower.yml
[root@bastion 0 ~/rhte-miniV master â­‘|âœš4â€¦2]# cd
----

** Explore the file `/root/ansible-tower.yml`.

+
[source,sh]
----
[root@bastion 0 ~]# less /root/ansible-tower.yml
----

** Sample Output

+
[source,yaml]
----
---
job_vars:
  job_vars:
  __meta__:
    callback:
      token: ""
      url: ""
    catalog:
      description: Install Ansible Tower
      namespace: ansible
      parameters:
      - description: Tower Version
        name: tower_version
        value: 3.5.0-1
      tags:
      - babylon
      - ansible
    deployer:
      entry_point: ansible/main.yml
      scm_ref: tower_worker_0
      scm_tag_prefix: tower_worker_0
      scm_type: git
      scm_url: https://github.com/redhat-cop/agnosticd.git
      type: agnosticd
    tower:
      action: deploy
      organization: gpte
      run_group: na
  HostedZoneId: Z7BVC200TM0XQ
  agnosticv_meta:
    agnosticd_git_repo: https://github.com/redhat-cop/agnosticd
          user: student_name
  aws_access_key_id: AajhdjaadasdasdasdashdjhasjdhaksK
  aws_secret_access_key: Ja;dlkas;lkda;lskd;laskdlakssdhn
  own_repo_path: http://d3s3zqyaz8cp2d.cloudfront.net/repos/tower
  platform: labs
  software_to_deploy: tower
  subdomain_base_suffix: .sandbox586.opentlc.com
  support_instance_count: 1
  tower_instance_count: 1
  worker_instance_count: 0
  guid: rhte2
----

=== Deploy `ANSIBLE_TOWER` config.

* Run `tower-cli` to deploy env with `-e` option to pass extra variables file `/root/ansible-tower.yml`.

[source,sh]
----
[root@bastion 0 ~]# tower-cli job launch --job-template=job-runner -e @/root/ansible-tower.yml
----

[TIP]
-vv for verbose output and --monitor allows you to monitor the job logs and look for errors. It is a good way to troubleshoot the issues.

* Explore Ansible Tower Web-UI `https://tower1.${GUID}.${BUID}.opentlc.com/`

** Login with user: `babylon` and password: `changeme`.

** Click on Projects from left side pane. You will able to see a new project with the name <type>-<scm_ref> for example `deploy-agnosticd-tower_worker_0`.

** Click on Templates a new job template is created for deployment with the name <action>-<type>-<scm_ref> for example `deploy-agnosticd-tower_worker_0`.

* You should expect the `tower-cli` command to fail.

* Sample output
+
[source,sh]
----
*** DETAILS: Requesting a copy of job standard output *************************
An exception occurred during task execution. To see the full traceback, use -vvv. The error was: tower_cli.exceptions.JobFailure: Job failed.
fatal: [localhost]: FAILED! => {"changed": false, "module_stderr": "Traceback (most recent call last):\\n  File \\"/var/lib/awx/.ansible/tmp/ansible-tmp-1568700965.79-145082337553393/AnsiballZ_tower_job_wait.py\\", line 114, in <module>\\n    _ansiballz_main()\\n  File \\"/var/lib/awx/.ansible/tmp/ansible-tmp-1568700965.79-145082337553393/AnsiballZ_tower_job_wait.py\\", line 106, in _ansiballz_main\\n    invoke_module(zipped_mod, temp_path, ANSIBALLZ_PARAMS)\\n  File \\"/var/lib/awx/.ansible/tmp/ansible-tmp-1568700965.79-145082337553393/AnsiballZ_tower_job_wait.py\\", line 49, in invoke_module\\n    imp.load_module('__main__', mod, module, MOD_DESC)\\n  File \\"/tmp/ansible_tower_job_wait_payload_n7VkBL/__main__.py\\", line 149, in <module>\\n  File \\"/tmp/ansible_tower_job_wait_payload_n7VkBL/__main__.py\\", line 127, in main\\n  File \\"/var/lib/awx/venv/ansible/lib/python2.7/site-packages/tower_cli/models/base.py\\", line 905, in monitor\\n    raise exc.JobFailure('Job failed.')\\ntower_cli.exceptions.JobFailure: Job failed.\\n", "module_stdout": "", "msg": "MODULE FAILURE\\nSee stdout/stderr for the exact error", "rc": 1}
----

=== Diagnose and troubleshoot the error.

** Go back to the browser to access Ansible Tower UI and click on Jobs and you will see two failed jobs `job-runner` and `deploy-agnosticd-tower_worker_0`.

** Click on `deploy-agnosticd-tower_worker_0` job to view the deployment logs. On the right side pane you should look for errors.

+
[source,text]
----
TASK [infra-ec2-template-generate : AWS Generate CloudFormation Template] ******
Tuesday 17 September 2019  06:16:12 +0000 (0:00:00.049)       0:00:03.690 *****
fatal: [localhost]: FAILED! => {"changed": false, "msg": "AnsibleUndefinedVariable: 'root_filesystem_size' is undefined"}
----

** `root_filesystem_size` value is not defined. You will need to define the size of root fiesystem for
  `cloudformation` template to be generated.

** Edit the file `/root/rhte-miniV/gpte/ANSIBLE_TOWER/dev.yaml` again to set the value of `root_filesystem_size` variable.

+
[source,sh]
----
[root@bastion 0 ~]# vi /root/rhte-miniV/gpte/ANSIBLE_TOWER/dev.yaml
software_to_deploy: tower
tower_instance_count: 1
support_instance_count: 1
worker_instance_count: 0
root_filesystem_size: 20
----

** Run the script `./babylon-merge.sh` to merge variables again.

+
[source,sh]
----
[root@bastion 0 ~]# cd /root/rhte-miniV/
[root@bastion 0 ~/rhte-miniV master â­‘|âœš4â€¦2]# ./babylon-merge.sh gpte ANSIBLE_TOWER dev > /root/ansible-tower.yml
[root@bastion 0 ~/rhte-miniV master â­‘|âœš4â€¦2]# cd
----

** Again Run `tower-cli` to deploy env with `-e` option to pass extra variables file `/root/ansible-tower.yml`.

+
[source,sh]
----
[root@bastion 0 ~]# tower-cli job launch --job-template=job-runner -e @/root/ansible-tower.yml -vvv --monitor
----

** Sample Output
+
[source,sh]
----
------End of Standard Out Stream--------
Resource changed.
== ============ =========================== ========== ========
id job_template           created             status   elapsed
== ============ =========================== ========== ========
18            7 2019-09-17T06:30:07.837010Z successful 1030.325
== ============ =========================== ========== ========
----

=== Verify the result

** Red Hat Ansible Tower is successfully deployed.

* To verify access the url `https://tower1.rhte2.${BUID}.opentlc.com` for example: `https://tower1.rhte2.sandbox712.opentlc.com/` using your browser.

** Login as user `rhte` and password `changeme`.

=== Congratulations!
